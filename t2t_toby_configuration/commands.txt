PROBLEM=predicting_questions_from_answers_twitter_entitiesAtEnd_tokens_16k
#PROBLEM=predicting_answers_from_questions_twitter_entitiesAtEnd_tokens_16k
MODEL=transformer
HPARAMS=transformer_toby

DATA_DIR=$HOME/t2t/toby/t2t_data
TMP_DIR=$HOME/t2t/toby/t2t_datagen
TRAIN_DIR=$HOME/t2t/toby/t2t_train/$PROBLEM/$MODEL-$HPARAMS

mkdir -p $DATA_DIR $TMP_DIR $TRAIN_DIR

# Generate data
t2t-datagen \
  --data_dir=$DATA_DIR \
  --tmp_dir=$TMP_DIR \
  --problem=$PROBLEM \
  --t2t_usr_dir=./t2t_toby_configuration

# Train
# *  If you run out of memory, add --hparams='batch_size=2048' or even 1024.


t2t-trainer \
  --data_dir=$DATA_DIR \
  --problems=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$TRAIN_DIR \
  --t2t_usr_dir=./t2t_toby_configuration

   # Decode

DECODE_FILE=$DATA_DIR/test_data_Q.txt


BEAM_SIZE=4
ALPHA=0.6

t2t-trainer \
  --data_dir=$DATA_DIR \
  --problems=$PROBLEM \
  --model=$MODEL \
  --hparams_set=$HPARAMS \
  --output_dir=$TRAIN_DIR \
  --train_steps=0 \
  --eval_steps=0 \
  --decode_beam_size=$BEAM_SIZE \
  --decode_alpha=$ALPHA \
  --decode_from_file=$DECODE_FILE \
  --t2t_usr_dir=./t2t_toby_configuration

cat $DECODE_FILE.$MODEL.$HPARAMS.beam$BEAM_SIZE.alpha$ALPHA.decodes